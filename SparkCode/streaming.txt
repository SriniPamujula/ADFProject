import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext
val conf =sc.getConf
// Create a local StreamingContext with two working thread and batch interval of 1 second.
// The master requires 2 cores to prevent from a starvation scenario.
import org.apache.spark.streaming.Seconds
//val conf = new SparkConf().setMaster("yarn").setAppName("NetworkWordCount")
val ssc = new StreamingContext(sc, Seconds(30))



var dataDirectory = "spark/sparkstreaming/newstream/"
var lines = streamingContext.textFileStream(dataDirectory)
case class intcc(ts:Int,gr:Int,vl:Int)
lines.foreachRDD{rdd=>
var mydf = rdd.map(y=>y.split(",")).map(y=>intcc(y(0).toInt,y(1).toInt,y(2).toInt)).toDF
mydf.groupBy("ts","gr").agg(sum("vl")).show
}

ssc.start

hdfs dfs -put x.txt spark/sparkstreaming/newstream/
hdfs dfs -put y.txt spark/sparkstreaming/newstream/