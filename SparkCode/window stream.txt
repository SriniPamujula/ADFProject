import org.apache.spark.streaming.Seconds;
import org.apache.spark.streaming.StreamingContext._;
import org.apache.spark.streaming.StreamingContext;
import org.apache.hadoop.io._;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
val ssc = new StreamingContext(sc, Seconds(30));
var dstr = ssc.fileStream[LongWritable, Text, TextInputFormat]("spark/sparkstreaming/onemorestream")

var wdstr = dstr.window(Seconds(60), Seconds(30))
var counter = 0;
wdstr.foreachRDD{
rdd =>
println("New Window Dstream")
sql("select current_timestamp").show(false)
println("Count of RDD")
println(rdd.count)
println("check no of lines")
rdd.map(x=>x._2.toString).filter(x=>x.split(",")(0).toString.equals(counter.toString)).map(x=>x.split(",")(1)).countByValue.foreach(println)
println(counter)
println("End of Window Dstream")
println("")
counter = counter + 1
}

ssc.start
ssc.awaitTermination


10:01 -- 10:01:30
counter =0

10:01:30 : 10:02
counter = 1

10:02 10:02:30
counter = 2

